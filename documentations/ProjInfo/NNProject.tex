\documentclass{article} 
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{fancyhdr} 
\usepackage{lastpage}
\usepackage[headsep=3em]{geometry}
\usepackage{array} 
\usepackage{comment} 

\pagestyle{fancy} 
\fancyhf{} 

\fancyhead[L]{LIF Neural Network Project Information}
\fancyhead[R]{\thepage of \pageref{LastPage} \\ Carson Crowley} 

\renewcommand{\thesection}{\Roman{section}}
%\title{Neural Network Project Information}
%\author{Carson Crowley}

\begin{document} 
%\maketitle

\begin{center} 
	\text{} 
	\\[22em] 
	{\LARGE \textbf{LIF Neural Network Project Information}} \\[1em] 
	{\large \textbf{June, 2025}} \\[0.5em] 
	{\small \textbf{Carson Crowley}} 
\end{center} 

\thispagestyle{empty}
\newpage

\section{Info}
\LaTeX documents will contain information regarding various aspects of this project. These documents will contain information on: 
\begin{itemize}
	\item How the Neural Network works. 
	\item Libraries the Neural Network utilizes. 
	\item The data the Neural Network is going to be trained on. 
	\item Build information. 
\end{itemize} 

\section*{Design Ideas}
\subsection*{Neuron and Neuron Clusters}
\textbf{Overview of Neuron}\\[0.5em]
A neuron acts as an I/O machine gaining input from data (\textit{if it is the first layer}) or neurons from higher-up layers. If a neuron recieves input from neurons in higher-up layers, randomized weights will be calculated for the synapse and used in the activation function for that current neuron (where j represenents the current neuron): 
\[V_j(t) = \sum_{i=1}^{n}x_i(t)\cdot \omega_{ij} - \text{leak}\]
where
\begin{itemize} 
	  \item	\(x_i(t) \in {0,1}\): spike from presynaptic neuron (in previous layer) at time \textit{t}
	  \item \(w_{ij}\): synaptic weight from neuron \textit{i} to \textit{j} 
\item \textit{leak} is a constant factor subtracted from the membrane to show decay.
\end{itemize}
		   
For the neuron to spike it must follow that: \\
\begin{equation} 
	\text{spike}_j(t) = 
	\begin{cases}
		1 & \text{if } V_j(t) \geq \theta \\ 
		0 & \text{otherwise} \\  
	\end{cases} 
\end{equation}
where \(\theta\) represents the \textit{threshold potential}. \\[0.5em] 

\textbf{Neuron Clusters and Associations}\\[1em]
If it follows that \(V_j(t) \geq \theta\), then the neuron \textit{i} or \textit{j} (or any neuron) will fire with a value of 1. The activation function represented by \(V_{\text{tag}}(t)\) represents the electrical charge accumulation within a neuron over time. If that charge surpasses a threshold (\(\theta\)), the neuron is thought to \textit{spike}.\\[0.5em] 
Neurons are held together by \textit{synapses} which act as bridges between a sender neuron and a reciever neuron. These bridges between neurons transmits an electrical signal between any sender and reciever, and contains a strength parameter to allow for associations to be formed. \\[1em] 
\textbf{An association} between any two neurons is modelled as a \textit{likelihood} for any neuron to fire after reciving input from a preceeding neuron. The activation function: \(V_{\text{j}}(t) = \sum_{i=1}^{n}x_i(t)\cdot \omega_{ij} - \text{leak}\) sums the product of the previous neuron (\textit{neuron i}) with the synaptic weight value between the neurons \text{i and j} (with the difference of a \textit{leak} factor). The stronger the \(\omega\) value is between neurons \textit{i and j}, the more likely neuron \textit{j} will fire subsequently. This weight factor allows any neuron to formulate an association to a pattern. So if neuron \textit{j} has a stronger \(\omega\) value with neurons \textit{i, k, and l}, neuron \textit{j} will, over time, serve the purpose of recognizing whatever pattern \textit{i, k, and l} represent.  
\end{document} 
